{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg,when,sum, round"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el archivo CSV mediante PySpark mediante la función read.csv de PySpark. Lo mostramos mediante la función \"show()\". Veremos que el conjunto de datos que nos muestra está desordenado, lo arreglamos en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('/FileStore/tables/datos.csv',sep = ';')\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a leer los datos pero con \"header=True para que sepa que contiene los nombres de las columnas. Implementamos \"inferSchema = True\" para que PySpark procese el tipo de datos de las columbas de acuerdo a los valores de las filas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('/FileStore/tables/datos.csv',sep = ';', header = True, inferSchema =True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos las 5 primeras filas\n",
    "display(df.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos mediante \"select\" las columnas \"Hospital\" y \"Facturacion\" agrupándolas por \"Hospital\" y calculamos la media de los valores de \"Facturación\". \n",
    "Después utilizamos la función \"groupby\" para agrupar los los datos del df por la columna \"Hospital\"\n",
    "\n",
    "Con la función \"agg\" agregamos la columna \"Facturacion\" con respecto a \"Hospital\"\n",
    "\n",
    "TL;DR: Calculamos la media de facturación de cada hospital y ordenamos los resultados segun el nombre del hospital\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.select(\"Hospital\",\"Facturacion\").groupBy(\"Hospital\").agg(avg(\"Facturacion\")).sort(\"Hospital\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos la estructura de los datos para ver su organización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como hay datos nulos. Veamos que podemos hacer con el comando .na.drop(how=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -how = 'any' elimina todas las filas en las que haya un valor null\n",
    "# -how = 'all' elimina las filas cuyos valores sean todos nulos\n",
    "\n",
    "\n",
    "df.na.drop(how = 'any').show(25) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También existe el parámetro thresh = n (int) el cual combinado con how = 'any' mostrará todos las filas que tenga n o más valores distintos de null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop(how = 'any', thresh = 2).show(25) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado podemos rellenar los missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill({'Name':'Roberto', 'Tiempo_en_hospital_horas':10, 'Edad':36}).show(25) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, vamos a sobreescribir la tabla de datos para eliminar los null para el resto del trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(how ='any')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear columnas nuevas\n",
    "Vamos a dividir la columna Tiempo_en_hospital_horas por 24 para calcular en una nueva columna la cantidad de dias que han estado los pacientes en el hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df.withColumn('Dias_en_el_hospital', round(df['Tiempo_en_hospital_horas']/24, 2)).show()\n",
    "df2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna Facturacion_por_hora en una nueva variable y redondeamos el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('Facturacion_por_hora', round(df2['Facturacion']/df2['Tiempo_en_hospital_horas'], 2))\n",
    "df3.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna Tiene seguro en una nueva variable y la convertimos en booleana con when(df3.Facturacion_por_hora<140, 0).otherwise(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.withColumn('Tiene_seguro', when(df3.Facturacion_por_hora<140, 0).otherwise(1))\n",
    "df4.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy con PySpark\n",
    "Funciona igual que con pandas. Vamos a ver cuantos pacientes han ido a cada hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Hospital').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos ahora el total de horas y de facturación por hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Hospital').sum('Tiempo_en_hospital_horas','Facturacion').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos por último la media de edad de los pacientes de cada hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Hospital').avg('Edad').show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
